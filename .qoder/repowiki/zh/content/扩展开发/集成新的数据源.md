# é›†æˆæ–°çš„æ•°æ®æº

<cite>
**æœ¬æ–‡æ¡£å¼•ç”¨çš„æ–‡ä»¶**
- [interface.py](file://tradingagents/dataflows/interface.py)
- [data_source_manager.py](file://tradingagents/dataflows/data_source_manager.py)
- [cache_manager.py](file://tradingagents/dataflows/cache_manager.py)
- [optimized_us_data.py](file://tradingagents/dataflows/optimized_us_data.py)
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py)
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py)
- [finnhub_utils.py](file://tradingagents/dataflows/finnhub_utils.py)
- [config.py](file://tradingagents/dataflows/config.py)
- [utils.py](file://tradingagents/dataflows/utils.py)
</cite>

## ç›®å½•
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [ç°æœ‰æ•°æ®æºåˆ†æ](#ç°æœ‰æ•°æ®æºåˆ†æ)
4. [æ•°æ®æ ‡å‡†åŒ–æµç¨‹](#æ•°æ®æ ‡å‡†åŒ–æµç¨‹)
5. [é€‚é…å™¨è®¾è®¡æ¨¡å¼](#é€‚é…å™¨è®¾è®¡æ¨¡å¼)
6. [ç¼“å­˜ç­–ç•¥](#ç¼“å­˜ç­–ç•¥)
7. [å¼‚å¸¸å¤„ç†æœºåˆ¶](#å¼‚å¸¸å¤„ç†æœºåˆ¶)
8. [æ–°æ•°æ®æºé›†æˆæŒ‡å—](#æ–°æ•°æ®æºé›†æˆæŒ‡å—)
9. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
10. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## æ¦‚è¿°

TradingAgents-CNç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„è®¾è®¡ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¥å£å±‚å’Œé€‚é…å™¨æ¨¡å¼æ”¯æŒå¤šç§é‡‘èæ•°æ®APIçš„é›†æˆã€‚æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜äº†å¦‚ä½•å°†æ–°çš„é‡‘èæ•°æ®APIæ— ç¼é›†æˆåˆ°ç³»ç»Ÿä¸­ï¼ŒåŒ…æ‹¬æ•°æ®æ ‡å‡†åŒ–ã€ç¼“å­˜ä¼˜åŒ–ã€é”™è¯¯å¤„ç†å’Œæ€§èƒ½ç›‘æ§ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

ç³»ç»Ÿçš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ï¼š
- **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰æ•°æ®æºé€šè¿‡æ ‡å‡†åŒ–æ¥å£æä¾›æœåŠ¡
- **é€‚é…å™¨æ¨¡å¼**ï¼šæ¯ä¸ªæ•°æ®æºéƒ½æœ‰ç‹¬ç«‹çš„é€‚é…å™¨å®ç°
- **ç¼“å­˜ä¼˜åŒ–**ï¼šæ™ºèƒ½ç¼“å­˜ç­–ç•¥å‡å°‘APIè°ƒç”¨å’Œæå‡å“åº”é€Ÿåº¦
- **é™çº§æœºåˆ¶**ï¼šå¤šå±‚çº§å¤‡ç”¨æ•°æ®æºç¡®ä¿æœåŠ¡è¿ç»­æ€§
- **æ€§èƒ½ç›‘æ§**ï¼šå®æ—¶ç›‘æ§å’Œæ—¥å¿—è®°å½•æ”¯æŒè¿ç»´ç®¡ç†

## ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
subgraph "ç”¨æˆ·æ¥å£å±‚"
A[ç»Ÿä¸€æ•°æ®æ¥å£]
B[æ•°æ®æºç®¡ç†å™¨]
end
subgraph "é€‚é…å™¨å±‚"
C[Tushareé€‚é…å™¨]
D[AkShareé€‚é…å™¨]
E[Finnhubé€‚é…å™¨]
F[è‡ªå®šä¹‰é€‚é…å™¨]
end
subgraph "æ•°æ®æºå±‚"
G[Tushare API]
H[AkShare API]
I[Finnhub API]
J[å…¶ä»–API]
end
subgraph "ç¼“å­˜å±‚"
K[æœ¬åœ°ç¼“å­˜]
L[æ•°æ®åº“ç¼“å­˜]
M[å†…å­˜ç¼“å­˜]
end
subgraph "é…ç½®ç®¡ç†å±‚"
N[é…ç½®ç®¡ç†å™¨]
O[æ—¥å¿—ç®¡ç†å™¨]
end
A --> B
B --> C
B --> D
B --> E
B --> F
C --> G
D --> H
E --> I
F --> J
B --> K
B --> L
B --> M
B --> N
B --> O
```

**å›¾è¡¨æ¥æº**
- [data_source_manager.py](file://tradingagents/dataflows/data_source_manager.py#L1-L50)
- [interface.py](file://tradingagents/dataflows/interface.py#L1-L100)

## ç°æœ‰æ•°æ®æºåˆ†æ

### Tushareæ•°æ®æº

Tushareæ˜¯ä¸­å›½é¢†å…ˆçš„é‡‘èæ•°æ®APIï¼Œæä¾›ä¸°å¯Œçš„Aè‚¡å¸‚åœºæ•°æ®ã€‚

```mermaid
classDiagram
class TushareDataAdapter {
+bool enable_cache
+TushareProvider provider
+CacheManager cache_manager
+get_stock_data(symbol, start_date, end_date) DataFrame
+get_stock_info(symbol) Dict
+search_stocks(keyword) DataFrame
+get_fundamentals(symbol) str
-_get_daily_data(symbol, start_date, end_date) DataFrame
-_get_realtime_data(symbol) DataFrame
-_validate_and_standardize_data(data) DataFrame
-_generate_fundamentals_report(symbol, stock_info, financial_data) str
}
class TushareProvider {
+bool connected
+get_stock_daily(symbol, start_date, end_date) DataFrame
+get_stock_info(symbol) Dict
+search_stocks(keyword) DataFrame
+get_financial_data(symbol) Dict
}
TushareDataAdapter --> TushareProvider : "ä½¿ç”¨"
```

**å›¾è¡¨æ¥æº**
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L25-L100)
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L150-L250)

### AkShareæ•°æ®æº

AkShareæ˜¯ä¸€ä¸ªå¼€æºçš„é‡‘èæ•°æ®æ¥å£åº“ï¼Œæ”¯æŒå¤šç§å¸‚åœºçš„è‚¡ç¥¨æ•°æ®è·å–ã€‚

```mermaid
classDiagram
class AKShareProvider {
+bool connected
+ak_module ak
+get_stock_data(symbol, start_date, end_date) DataFrame
+get_stock_info(symbol) Dict
+get_hk_stock_data(symbol, start_date, end_date) DataFrame
+get_hk_stock_info(symbol) Dict
+get_financial_data(symbol) Dict
-_normalize_hk_symbol_for_akshare(symbol) str
-_configure_timeout() void
}
AKShareProvider --> AKShareProvider : "æ•°æ®è·å–"
```

**å›¾è¡¨æ¥æº**
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py#L15-L50)
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py#L100-L200)

### Finnhubæ•°æ®æº

Finnhubæä¾›å…¨çƒè‚¡å¸‚æ•°æ®ï¼Œç‰¹åˆ«é€‚åˆç¾è‚¡å’Œå›½é™…å¸‚åœºçš„æ•°æ®éœ€æ±‚ã€‚

```mermaid
sequenceDiagram
participant Client as å®¢æˆ·ç«¯
participant Manager as æ•°æ®æºç®¡ç†å™¨
participant Finnhub as Finnhubé€‚é…å™¨
participant API as Finnhub API
Client->>Manager : è¯·æ±‚è‚¡ç¥¨æ•°æ®
Manager->>Finnhub : è°ƒç”¨æ•°æ®è·å–æ–¹æ³•
Finnhub->>API : å‘é€APIè¯·æ±‚
API-->>Finnhub : è¿”å›JSONæ•°æ®
Finnhub->>Finnhub : æ•°æ®è§£æå’Œè½¬æ¢
Finnhub-->>Manager : è¿”å›æ ‡å‡†åŒ–æ•°æ®
Manager-->>Client : è¿”å›æ ¼å¼åŒ–ç»“æœ
Note over Client,API : æ”¯æŒç¼“å­˜å’Œé”™è¯¯é‡è¯•æœºåˆ¶
```

**å›¾è¡¨æ¥æº**
- [finnhub_utils.py](file://tradingagents/dataflows/finnhub_utils.py#L10-L50)
- [optimized_us_data.py](file://tradingagents/dataflows/optimized_us_data.py#L50-L150)

**ç« èŠ‚æ¥æº**
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L1-L100)
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py#L1-L100)
- [finnhub_utils.py](file://tradingagents/dataflows/finnhub_utils.py#L1-L57)

## æ•°æ®æ ‡å‡†åŒ–æµç¨‹

ç³»ç»Ÿé‡‡ç”¨ä¸¥æ ¼çš„æ•°æ®æ ‡å‡†åŒ–æµç¨‹ï¼Œç¡®ä¿ä¸åŒæ•°æ®æºçš„æ•°æ®æ ¼å¼ä¸€è‡´æ€§ã€‚

### æ ‡å‡†åŒ–å­—æ®µæ˜ å°„

| æ ‡å‡†å­—æ®µ | Tushare | AkShare | Finnhub | è‡ªå®šä¹‰ |
|---------|---------|---------|---------|--------|
| æ—¥æœŸ | trade_date | æ—¥æœŸ | date | date |
| å¼€ç›˜ä»· | open | å¼€ç›˜ | open | open |
| æ”¶ç›˜ä»· | close | æ”¶ç›˜ | close | close |
| æœ€é«˜ä»· | high | æœ€é«˜ | high | high |
| æœ€ä½ä»· | low | æœ€ä½ | low | low |
| æˆäº¤é‡ | vol | æˆäº¤é‡ | volume | volume |
| æˆäº¤é¢ | amount | æˆäº¤é¢ | amount | amount |
| è‚¡ç¥¨ä»£ç  | ts_code | code | symbol | symbol |

### æ•°æ®éªŒè¯æœºåˆ¶

```mermaid
flowchart TD
A[æ¥æ”¶åŸå§‹æ•°æ®] --> B{æ•°æ®å®Œæ•´æ€§æ£€æŸ¥}
B --> |é€šè¿‡| C[åˆ—åæ ‡å‡†åŒ–]
B --> |å¤±è´¥| D[æ·»åŠ å¤‡ç”¨åˆ—]
C --> E[æ•°æ®ç±»å‹éªŒè¯]
E --> F{æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥}
F --> |é€šè¿‡| G[åº”ç”¨ä¸šåŠ¡è§„åˆ™]
F --> |å¤±è´¥| H[æ ‡è®°ä¸ºæ— æ•ˆ]
D --> I[æ•°æ®å¡«å……]
I --> E
G --> J[ç”Ÿæˆæ ‡å‡†åŒ–æ•°æ®]
H --> K[è¿”å›é”™è¯¯ä¿¡æ¯]
J --> L[ç¼“å­˜å¤„ç†]
K --> M[é™çº§å¤„ç†]
```

**å›¾è¡¨æ¥æº**
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L200-L300)
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py#L300-L400)

**ç« èŠ‚æ¥æº**
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L200-L400)
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py#L300-L500)

## é€‚é…å™¨è®¾è®¡æ¨¡å¼

### é€‚é…å™¨æ¥å£è§„èŒƒ

æ¯ä¸ªæ•°æ®æºé€‚é…å™¨å¿…é¡»å®ç°ä»¥ä¸‹æ ¸å¿ƒæ¥å£ï¼š

```python
class BaseDataAdapter:
    def get_stock_data(self, symbol: str, start_date: str, end_date: str) -> Union[pd.DataFrame, str]:
        """è·å–è‚¡ç¥¨å†å²æ•°æ®"""
        pass
    
    def get_stock_info(self, symbol: str) -> Dict:
        """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        pass
    
    def search_stocks(self, keyword: str) -> pd.DataFrame:
        """æœç´¢è‚¡ç¥¨"""
        pass
    
    def get_fundamentals(self, symbol: str) -> str:
        """è·å–åŸºæœ¬é¢æ•°æ®"""
        pass
```

### é€‚é…å™¨å®ç°æ¨¡æ¿

ä»¥ä¸‹æ˜¯æ–°æ•°æ®æºé€‚é…å™¨çš„æ ‡å‡†å®ç°æ¨¡æ¿ï¼š

```python
class CustomDataAdapter:
    def __init__(self, enable_cache: bool = True):
        self.enable_cache = enable_cache
        self.provider = self._initialize_provider()
        self.cache_manager = self._initialize_cache()
    
    def _initialize_provider(self):
        """åˆå§‹åŒ–æ•°æ®æä¾›å™¨"""
        try:
            # å®ç°å…·ä½“çš„APIå®¢æˆ·ç«¯åˆå§‹åŒ–
            return CustomProvider()
        except Exception as e:
            logger.error(f"âŒ è‡ªå®šä¹‰æ•°æ®æºåˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def get_stock_data(self, symbol: str, start_date: str, end_date: str) -> Union[pd.DataFrame, str]:
        """è·å–è‚¡ç¥¨æ•°æ®çš„ä¸»è¦å®ç°"""
        # å®ç°æ•°æ®è·å–é€»è¾‘
        pass
```

**ç« èŠ‚æ¥æº**
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L25-L100)
- [akshare_utils.py](file://tradingagents/dataflows/akshare_utils.py#L15-L50)

## ç¼“å­˜ç­–ç•¥

### å¤šå±‚ç¼“å­˜æ¶æ„

```mermaid
graph TB
subgraph "ç¼“å­˜å±‚æ¬¡ç»“æ„"
A[å†…å­˜ç¼“å­˜] --> B[æœ¬åœ°æ–‡ä»¶ç¼“å­˜]
B --> C[æ•°æ®åº“ç¼“å­˜]
end
subgraph "ç¼“å­˜ç­–ç•¥"
D[æŒ‰å¸‚åœºåˆ†ç±»ç¼“å­˜]
E[æŒ‰æ•°æ®ç±»å‹åˆ†ç±»ç¼“å­˜]
F[æ™ºèƒ½TTLç®¡ç†]
end
subgraph "ç¼“å­˜é…ç½®"
G[ç¾è‚¡æ•°æ®: 2å°æ—¶]
H[Aè‚¡æ•°æ®: 1å°æ—¶]
I[æ–°é—»æ•°æ®: 6å°æ—¶]
J[åŸºæœ¬é¢æ•°æ®: 24å°æ—¶]
end
A --> D
B --> E
C --> F
D --> G
E --> H
F --> I
F --> J
```

**å›¾è¡¨æ¥æº**
- [cache_manager.py](file://tradingagents/dataflows/cache_manager.py#L50-L150)

### ç¼“å­˜é”®ç”Ÿæˆç­–ç•¥

ç¼“å­˜é”®é‡‡ç”¨åŸºäºå‚æ•°çš„å“ˆå¸Œç®—æ³•ï¼Œç¡®ä¿å”¯ä¸€æ€§å’Œå¯è¿½æº¯æ€§ï¼š

```python
def _generate_cache_key(self, data_type: str, symbol: str, **kwargs) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    params_str = f"{data_type}_{symbol}"
    for key, value in sorted(kwargs.items()):
        params_str += f"_{key}_{value}"
    
    cache_key = hashlib.md5(params_str.encode()).hexdigest()[:12]
    return f"{symbol}_{data_type}_{cache_key}"
```

### ç¼“å­˜å¤±æ•ˆæœºåˆ¶

ç³»ç»Ÿå®ç°äº†æ™ºèƒ½çš„ç¼“å­˜å¤±æ•ˆç­–ç•¥ï¼š

- **æ—¶é—´é©±åŠ¨å¤±æ•ˆ**ï¼šåŸºäºé…ç½®çš„TTLï¼ˆç”Ÿå­˜æ—¶é—´ï¼‰
- **å†…å®¹é©±åŠ¨å¤±æ•ˆ**ï¼šåŸºäºæ•°æ®è´¨é‡å’Œå®Œæ•´æ€§æ£€æŸ¥
- **æ‰‹åŠ¨åˆ·æ–°**ï¼šæ”¯æŒå¼ºåˆ¶åˆ·æ–°ç¼“å­˜çš„åŠŸèƒ½

**ç« èŠ‚æ¥æº**
- [cache_manager.py](file://tradingagents/dataflows/cache_manager.py#L1-L200)
- [optimized_us_data.py](file://tradingagents/dataflows/optimized_us_data.py#L100-L200)

## å¼‚å¸¸å¤„ç†æœºåˆ¶

### å¤šå±‚çº§å¼‚å¸¸å¤„ç†

```mermaid
flowchart TD
A[APIè°ƒç”¨] --> B{ç½‘ç»œè¿æ¥æ£€æŸ¥}
B --> |å¤±è´¥| C[ç½‘ç»œå¼‚å¸¸å¤„ç†]
B --> |æˆåŠŸ| D[APIè¯·æ±‚]
D --> E{HTTPçŠ¶æ€ç }
E --> |4xx| F[å®¢æˆ·ç«¯é”™è¯¯å¤„ç†]
E --> |5xx| G[æœåŠ¡å™¨é”™è¯¯å¤„ç†]
E --> |200| H[æ•°æ®è§£æ]
H --> I{æ•°æ®æœ‰æ•ˆæ€§}
I --> |æ— æ•ˆ| J[æ•°æ®æ¸…æ´—å¤„ç†]
I --> |æœ‰æ•ˆ| K[è¿”å›ç»“æœ]
C --> L[é™çº§å¤„ç†]
F --> L
G --> L
J --> L
L --> M[å¤‡ç”¨æ•°æ®æº]
M --> N{å¤‡ç”¨æºå¯ç”¨}
N --> |æ˜¯| O[å¤‡ç”¨æ•°æ®è·å–]
N --> |å¦| P[è¿”å›é”™è¯¯ä¿¡æ¯]
O --> Q[æ•°æ®æ ‡å‡†åŒ–]
Q --> K
```

**å›¾è¡¨æ¥æº**
- [data_source_manager.py](file://tradingagents/dataflows/data_source_manager.py#L300-L400)

### é™çº§æœºåˆ¶å®ç°

ç³»ç»Ÿå®ç°äº†æ™ºèƒ½çš„é™çº§æœºåˆ¶ï¼Œç¡®ä¿æœåŠ¡çš„é«˜å¯ç”¨æ€§ï¼š

```python
def _try_fallback_sources(self, symbol: str, start_date: str, end_date: str) -> str:
    """å°è¯•å¤‡ç”¨æ•°æ®æº - é¿å…é€’å½’è°ƒç”¨"""
    fallback_order = [
        ChinaDataSource.AKSHARE,
        ChinaDataSource.TUSHARE,
        ChinaDataSource.BAOSTOCK
    ]
    
    for source in fallback_order:
        if source != self.current_source and source in self.available_sources:
            try:
                # ç›´æ¥è°ƒç”¨å…·ä½“çš„æ•°æ®æºæ–¹æ³•ï¼Œé¿å…é€’å½’
                if source == ChinaDataSource.TUSHARE:
                    result = self._get_tushare_data(symbol, start_date, end_date)
                elif source == ChinaDataSource.AKSHARE:
                    result = self._get_akshare_data(symbol, start_date, end_date)
                # ... å…¶ä»–æ•°æ®æº
                return result
            except Exception as e:
                continue
    
    return f"âŒ æ‰€æœ‰æ•°æ®æºéƒ½æ— æ³•è·å–{symbol}çš„æ•°æ®"
```

**ç« èŠ‚æ¥æº**
- [data_source_manager.py](file://tradingagents/dataflows/data_source_manager.py#L400-L500)

## æ–°æ•°æ®æºé›†æˆæŒ‡å—

### ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºé€‚é…å™¨ç±»

1. **ç»§æ‰¿åŸºç¡€æ¥å£**ï¼šå®ç°æ ‡å‡†çš„æ•°æ®æºæ¥å£
2. **åˆå§‹åŒ–é…ç½®**ï¼šè®¾ç½®APIå¯†é’¥å’ŒåŸºç¡€URL
3. **é”™è¯¯å¤„ç†**ï¼šå®ç°å¼‚å¸¸æ•è·å’Œé‡è¯•æœºåˆ¶

```python
class NewDataSourceAdapter:
    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or os.getenv('NEW_DATA_API_KEY')
        self.base_url = base_url or 'https://api.newdatasource.com/v1'
        self.session = self._create_session()
        self.cache_enabled = True
    
    def _create_session(self):
        """åˆ›å»ºHTTPä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        })
        return session
```

### ç¬¬äºŒæ­¥ï¼šå®ç°æ•°æ®è·å–æ–¹æ³•

```python
def get_stock_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
    """è·å–è‚¡ç¥¨å†å²æ•°æ®"""
    try:
        # æ„å»ºAPIè¯·æ±‚URL
        endpoint = f"{self.base_url}/stocks/{symbol}/history"
        params = {
            'start_date': start_date,
            'end_date': end_date,
            'interval': 'daily'
        }
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_enabled:
            cache_key = self._generate_cache_key('stock_data', symbol, start_date=start_date, end_date=end_date)
            cached_data = self._load_from_cache(cache_key)
            if cached_data is not None:
                return cached_data
        
        # å‘é€APIè¯·æ±‚
        response = self.session.get(endpoint, params=params, timeout=30)
        response.raise_for_status()
        
        # è§£æå“åº”æ•°æ®
        raw_data = response.json()
        df = self._parse_stock_data(raw_data)
        
        # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
        standardized_data = self._standardize_data(df)
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if self.cache_enabled:
            self._save_to_cache(cache_key, standardized_data)
        
        return standardized_data
        
    except requests.RequestException as e:
        logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return pd.DataFrame()
```

### ç¬¬ä¸‰æ­¥ï¼šå®ç°æ•°æ®æ ‡å‡†åŒ–

```python
def _standardize_data(self, data: pd.DataFrame) -> pd.DataFrame:
    """æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""
    if data.empty:
        return data
    
    # åˆ—åæ˜ å°„
    column_mapping = {
        'date': 'date',
        'open': 'open',
        'close': 'close',
        'high': 'high',
        'low': 'low',
        'volume': 'volume',
        'symbol': 'code'
    }
    
    # é‡å‘½ååˆ—
    data = data.rename(columns=column_mapping)
    
    # éªŒè¯å¿…è¦åˆ—
    required_columns = ['date', 'open', 'close', 'high', 'low', 'volume']
    missing_columns = [col for col in required_columns if col not in data.columns]
    
    if missing_columns:
        logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        return pd.DataFrame()
    
    # æ•°æ®ç±»å‹è½¬æ¢
    data['date'] = pd.to_datetime(data['date'])
    numeric_columns = ['open', 'close', 'high', 'low', 'volume']
    for col in numeric_columns:
        data[col] = pd.to_numeric(data[col], errors='coerce')
    
    return data.sort_values('date').reset_index(drop=True)
```

### ç¬¬å››æ­¥ï¼šæ³¨å†Œæ•°æ®æºç®¡ç†å™¨

```python
class ChinaDataSource(Enum):
    """ä¸­å›½è‚¡ç¥¨æ•°æ®æºæšä¸¾"""
    TUSHARE = "tushare"
    AKSHARE = "akshare"
    BAOSTOCK = "baostock"
    NEW_DATA = "new_data"  # æ–°å¢æ•°æ®æº

class DataSourceManager:
    def __init__(self):
        # ... ç°æœ‰åˆå§‹åŒ–ä»£ç 
        self._register_new_data_source()
    
    def _register_new_data_source(self):
        """æ³¨å†Œæ–°çš„æ•°æ®æº"""
        try:
            from .new_data_adapter import NewDataSourceAdapter
            self.new_data_adapter = NewDataSourceAdapter()
            self.available_sources.append(ChinaDataSource.NEW_DATA)
            logger.info("âœ… æ–°æ•°æ®æºæ³¨å†ŒæˆåŠŸ")
        except ImportError as e:
            logger.warning(f"âš ï¸ æ–°æ•°æ®æºæ³¨å†Œå¤±è´¥: {e}")
    
    def get_stock_data(self, symbol: str, start_date: str = None, end_date: str = None) -> str:
        """è·å–è‚¡ç¥¨æ•°æ®çš„ç»Ÿä¸€æ¥å£"""
        # æ ¹æ®å½“å‰æ•°æ®æºè°ƒç”¨ç›¸åº”çš„æ–¹æ³•
        if self.current_source == ChinaDataSource.NEW_DATA:
            return self._get_new_data(symbol, start_date, end_date)
        # ... å…¶ä»–æ•°æ®æºå¤„ç†
```

### ç¬¬äº”æ­¥ï¼šé…ç½®å’Œæµ‹è¯•

1. **ç¯å¢ƒå˜é‡é…ç½®**ï¼šè®¾ç½®APIå¯†é’¥å’ŒåŸºç¡€URL
2. **å•å…ƒæµ‹è¯•**ï¼šç¼–å†™å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹
3. **é›†æˆæµ‹è¯•**ï¼šéªŒè¯ä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§
4. **æ€§èƒ½æµ‹è¯•**ï¼šè¯„ä¼°APIè°ƒç”¨é¢‘ç‡å’Œå“åº”æ—¶é—´

```python
# æµ‹è¯•è„šæœ¬ç¤ºä¾‹
def test_new_data_source():
    """æµ‹è¯•æ–°æ•°æ®æºåŠŸèƒ½"""
    adapter = NewDataSourceAdapter(api_key="test_key")
    
    # æµ‹è¯•æ•°æ®è·å–
    data = adapter.get_stock_data("AAPL", "2024-01-01", "2024-01-31")
    assert not data.empty, "æ•°æ®è·å–å¤±è´¥"
    
    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    cached_data = adapter.get_stock_data("AAPL", "2024-01-01", "2024-01-31")
    assert len(data) == len(cached_data), "ç¼“å­˜æ•°æ®ä¸ä¸€è‡´"
    
    print("âœ… æ–°æ•°æ®æºæµ‹è¯•é€šè¿‡")
```

**ç« èŠ‚æ¥æº**
- [tushare_adapter.py](file://tradingagents/dataflows/tushare_adapter.py#L25-L150)
- [data_source_manager.py](file://tradingagents/dataflows/data_source_manager.py#L50-L200)

## æœ€ä½³å®è·µ

### 1. APIè°ƒç”¨ä¼˜åŒ–

- **æ‰¹é‡è¯·æ±‚**ï¼šå°½å¯èƒ½ä½¿ç”¨æ‰¹é‡APIå‡å°‘è°ƒç”¨æ¬¡æ•°
- **å¹¶å‘æ§åˆ¶**ï¼šå®ç°åˆç†çš„å¹¶å‘é™åˆ¶é¿å…APIé™æµ
- **æŒ‡æ•°é€€é¿**ï¼šåœ¨é‡è¯•æ—¶ä½¿ç”¨æŒ‡æ•°é€€é¿ç®—æ³•

```python
def _make_request_with_retry(self, url: str, params: dict = None, max_retries: int = 3):
    """å¸¦é‡è¯•æœºåˆ¶çš„APIè¯·æ±‚"""
    for attempt in range(max_retries):
        try:
            response = self.session.get(url, params=params, timeout=30)
            if response.status_code == 429:  # Too Many Requests
                sleep_time = 2 ** attempt
                time.sleep(sleep_time)
                continue
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)
```

### 2. æ•°æ®è´¨é‡ä¿è¯

- **å®Œæ•´æ€§æ£€æŸ¥**ï¼šéªŒè¯å…³é”®å­—æ®µçš„å­˜åœ¨å’Œæœ‰æ•ˆæ€§
- **å¼‚å¸¸å€¼æ£€æµ‹**ï¼šè¯†åˆ«å’Œå¤„ç†å¼‚å¸¸çš„ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
- **æ—¶é—´åºåˆ—éªŒè¯**ï¼šç¡®ä¿æ•°æ®çš„æ—¶é—´é¡ºåºæ­£ç¡®

```python
def _validate_data_quality(self, data: pd.DataFrame) -> bool:
    """éªŒè¯æ•°æ®è´¨é‡"""
    if data.empty:
        return False
    
    # æ£€æŸ¥å…³é”®å­—æ®µ
    required_fields = ['date', 'open', 'close', 'high', 'low', 'volume']
    for field in required_fields:
        if field not in data.columns:
            logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
            return False
    
    # æ£€æŸ¥ä»·æ ¼åˆç†æ€§
    invalid_prices = data[(data['high'] <= data['low']) | 
                         (data['close'] <= 0) | 
                         (data['volume'] < 0)]
    if len(invalid_prices) > 0:
        logger.warning(f"å‘ç°{len(invalid_prices)}æ¡å¼‚å¸¸æ•°æ®")
        return False
    
    return True
```

### 3. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

- **æ™ºèƒ½TTL**ï¼šæ ¹æ®æ•°æ®ç±»å‹åŠ¨æ€è°ƒæ•´ç¼“å­˜æ—¶é—´
- **é¢„çƒ­æœºåˆ¶**ï¼šåœ¨é«˜å³°æ—¶æ®µæå‰åŠ è½½å¸¸ç”¨æ•°æ®
- **å‹ç¼©å­˜å‚¨**ï¼šå¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œå‹ç¼©å­˜å‚¨

```python
def _get_cache_ttl(self, data_type: str, symbol: str) -> int:
    """è·å–ç¼“å­˜TTLï¼ˆå°æ—¶ï¼‰"""
    market_type = self._determine_market_type(symbol)
    ttl_config = {
        'stock_data': 1 if market_type == 'china' else 2,
        'news': 6,
        'fundamentals': 24
    }
    return ttl_config.get(data_type, 24)
```

### 4. ç›‘æ§å’Œæ—¥å¿—

- **æ€§èƒ½æŒ‡æ ‡**ï¼šè®°å½•APIå“åº”æ—¶é—´å’ŒæˆåŠŸç‡
- **é”™è¯¯è¿½è¸ª**ï¼šè¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯å’Œä¸Šä¸‹æ–‡
- **ä½¿ç”¨ç»Ÿè®¡**ï¼šè·Ÿè¸ªå„æ•°æ®æºçš„ä½¿ç”¨æƒ…å†µ

```python
def _log_api_call(self, symbol: str, data_type: str, duration: float, success: bool):
    """è®°å½•APIè°ƒç”¨æ—¥å¿—"""
    logger.info(f"ğŸ“Š APIè°ƒç”¨ç»Ÿè®¡",
               extra={
                   'symbol': symbol,
                   'data_type': data_type,
                   'duration': duration,
                   'success': success,
                   'timestamp': datetime.now().isoformat(),
                   'event_type': 'api_call'
               })
```

**ç« èŠ‚æ¥æº**
- [optimized_us_data.py](file://tradingagents/dataflows/optimized_us_data.py#L200-L300)
- [cache_manager.py](file://tradingagents/dataflows/cache_manager.py#L400-L500)

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. APIè®¤è¯å¤±è´¥

**é—®é¢˜ç—‡çŠ¶**ï¼šå‡ºç°401 Unauthorizedé”™è¯¯
**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®
- éªŒè¯å¯†é’¥æƒé™æ˜¯å¦è¶³å¤Ÿ
- ç¡®è®¤å¯†é’¥æœªè¿‡æœŸ

```python
def _check_api_credentials(self) -> bool:
    """æ£€æŸ¥APIå‡­æ®"""
    if not self.api_key:
        logger.error("âŒ APIå¯†é’¥æœªé…ç½®")
        return False
    
    try:
        # å‘é€æµ‹è¯•è¯·æ±‚
        response = self.session.get(f"{self.base_url}/test", timeout=10)
        if response.status_code == 401:
            logger.error("âŒ APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥")
            return False
        return True
    except Exception as e:
        logger.error(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False
```

#### 2. æ•°æ®æ ¼å¼ä¸å…¼å®¹

**é—®é¢˜ç—‡çŠ¶**ï¼šæ•°æ®æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯
**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥APIå“åº”æ ¼å¼
- æ›´æ–°åˆ—åæ˜ å°„è¡¨
- æ·»åŠ æ•°æ®æ¸…æ´—é€»è¾‘

```python
def _detect_data_format(self, raw_data: dict) -> str:
    """æ£€æµ‹æ•°æ®æ ¼å¼"""
    if 'timestamp' in raw_data and 'price' in raw_data:
        return 'format_a'
    elif 'date' in raw_data and 'close' in raw_data:
        return 'format_b'
    else:
        return 'unknown'
```

#### 3. ç¼“å­˜é—®é¢˜

**é—®é¢˜ç—‡çŠ¶**ï¼šç¼“å­˜æ•°æ®ä¸å‡†ç¡®æˆ–æ— æ³•åŠ è½½
**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ç¼“å­˜ç›®å½•æƒé™
- éªŒè¯ç¼“å­˜æ–‡ä»¶å®Œæ•´æ€§
- æ¸…ç†æŸåçš„ç¼“å­˜æ–‡ä»¶

```python
def _verify_cache_integrity(self, cache_key: str) -> bool:
    """éªŒè¯ç¼“å­˜å®Œæ•´æ€§"""
    metadata = self._load_metadata(cache_key)
    if not metadata:
        return False
    
    data_file = Path(metadata['file_path'])
    if not data_file.exists():
        return False
    
    try:
        # å°è¯•åŠ è½½æ•°æ®
        if metadata['file_format'] == 'csv':
            pd.read_csv(data_file)
        else:
            with open(data_file, 'r') as f:
                json.load(f)
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸå: {e}")
        return False
```

#### 4. æ€§èƒ½é—®é¢˜

**é—®é¢˜ç—‡çŠ¶**ï¼šAPIå“åº”ç¼“æ…¢æˆ–è¶…æ—¶
**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä¼˜åŒ–æŸ¥è¯¢å‚æ•°
- å®ç°åˆ†é¡µåŠ è½½
- ä½¿ç”¨å¼‚æ­¥è¯·æ±‚

```python
async def _async_get_stock_data(self, symbol: str, date_range: List[str]):
    """å¼‚æ­¥è·å–è‚¡ç¥¨æ•°æ®"""
    tasks = []
    for date_chunk in self._split_date_range(date_range):
        task = asyncio.create_task(
            self._fetch_data_chunk(symbol, date_chunk)
        )
        tasks.append(task)
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return self._merge_results(results)
```

### è°ƒè¯•å·¥å…·

ç³»ç»Ÿæä¾›äº†ä¸°å¯Œçš„è°ƒè¯•å·¥å…·å¸®åŠ©å¼€å‘è€…è¯Šæ–­é—®é¢˜ï¼š

```python
def debug_data_source(self, symbol: str, start_date: str, end_date: str):
    """è°ƒè¯•æ•°æ®æºé—®é¢˜"""
    print(f"ğŸ” è°ƒè¯•æ•°æ®æº: {symbol}")
    print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
    
    # æ£€æŸ¥å¯ç”¨æ•°æ®æº
    print(f"ğŸ“Š å¯ç”¨æ•°æ®æº: {self.available_sources}")
    
    # æµ‹è¯•æ¯ä¸ªæ•°æ®æº
    for source in self.available_sources:
        print(f"\nğŸ”„ æµ‹è¯• {source.value}...")
        try:
            result = self.get_stock_data(symbol, start_date, end_date)
            print(f"   âœ… æˆåŠŸ: {len(result)} æ¡æ•°æ®")
            print(f"   ğŸ“… æœ€æ–°æ—¥æœŸ: {result['date'].max()}")
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
```

**ç« èŠ‚æ¥æº**
- [data_source_manager.py](file://tradingagents/dataflows/data_source_manager.py#L600-L700)
- [cache_manager.py](file://tradingagents/dataflows/cache_manager.py#L500-L600)

## ç»“è®º

é€šè¿‡æœ¬æŒ‡å—ï¼Œå¼€å‘è€…å¯ä»¥ç³»ç»Ÿåœ°å°†æ–°çš„é‡‘èæ•°æ®APIé›†æˆåˆ°TradingAgents-CNç³»ç»Ÿä¸­ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

1. **éµå¾ªæ ‡å‡†åŒ–æ¥å£**ï¼šç¡®ä¿æ–°æ•°æ®æºç¬¦åˆç³»ç»Ÿçš„ç»Ÿä¸€æ¥å£è§„èŒƒ
2. **å®ç°å®Œæ•´çš„é€‚é…å™¨**ï¼šåŒ…æ‹¬æ•°æ®è·å–ã€ç¼“å­˜ã€é”™è¯¯å¤„ç†ç­‰åŠŸèƒ½
3. **ä¼˜åŒ–æ€§èƒ½å’Œå¯é æ€§**ï¼šé€šè¿‡ç¼“å­˜ç­–ç•¥å’Œé™çº§æœºåˆ¶æå‡ç³»ç»Ÿç¨³å®šæ€§
4. **å®Œå–„çš„æµ‹è¯•è¦†ç›–**ï¼šç¡®ä¿æ–°åŠŸèƒ½çš„è´¨é‡å’Œå…¼å®¹æ€§
5. **æŒç»­ç›‘æ§å’Œç»´æŠ¤**ï¼šå»ºç«‹æœ‰æ•ˆçš„ç›‘æ§ä½“ç³»åŠæ—¶å‘ç°å’Œè§£å†³é—®é¢˜

éšç€é‡‘èæ•°æ®å¸‚åœºçš„ä¸æ–­å‘å±•ï¼Œç³»ç»Ÿå°†ç»§ç»­æ¼”è¿›ä»¥æ”¯æŒæ›´å¤šçš„æ•°æ®æºå’Œæ›´å¤æ‚çš„éœ€æ±‚ã€‚å¼€å‘è€…åº”å…³æ³¨ç³»ç»Ÿçš„æ›´æ–°å’Œæ”¹è¿›ï¼ŒåŠæ—¶å‡çº§é€‚é…å™¨ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚